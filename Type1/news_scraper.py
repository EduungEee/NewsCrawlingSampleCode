"""
ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ê´€ë ¨ ê¸°ëŠ¥
"""
import requests
from bs4 import BeautifulSoup
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
from database import NewsDatabase

class NewsScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
    def get_news_by_category(self, category, source_name=None):
        """ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
        try:
            # DBì—ì„œ ë‰´ìŠ¤ ì†ŒìŠ¤ í™•ì¸
            db = NewsDatabase()
            sources = db.get_news_sources(category)
            
            if not sources:
                return self._get_sample_news(category)
            
            # íŠ¹ì • ì†ŒìŠ¤ê°€ ì§€ì •ëœ ê²½ìš°
            if source_name:
                sources = [s for s in sources if s['source_name'] == source_name]
            
            if not sources:
                return self._get_sample_news(category)
            
            # ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘
            all_news = []
            for source in sources:
                try:
                    news_list = self._scrape_from_source(source, category)
                    all_news.extend(news_list)
                except Exception as e:
                    print(f"{source['source_name']} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
                    continue
            
            return all_news if all_news else self._get_sample_news(category)
            
        except Exception as e:
            print(f"ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return self._get_sample_news(category)
    
    def _scrape_from_source(self, source, category):
        """íŠ¹ì • ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘"""
        try:
            print(f"ğŸ” {source['source_name']}ì—ì„œ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")
            
            # 1ë‹¨ê³„: requests + BeautifulSoup ì‹œë„
            news_list = self._scrape_with_requests(source, category)
            if news_list:
                print(f"âœ… requestsë¡œ {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì„±ê³µ")
                return news_list
            
            # 2ë‹¨ê³„: Selenium ì‹œë„
            news_list = self._scrape_with_selenium(source, category)
            if news_list:
                print(f"âœ… Seleniumìœ¼ë¡œ {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì„±ê³µ")
                return news_list
            
            print(f"âŒ {source['source_name']}ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨")
            return []
            
        except Exception as e:
            print(f"âŒ ì†ŒìŠ¤ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []
    
    def _scrape_with_requests(self, source, category):
        """requestsë¥¼ ì‚¬ìš©í•œ ìŠ¤í¬ë˜í•‘"""
        try:
            url = source['url']
            print(f"ğŸ“¡ {url}ì— ìš”ì²­ ì¤‘...")
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            print(f"âœ… HTTP ì‘ë‹µ ì„±ê³µ: {response.status_code}")
            
            soup = BeautifulSoup(response.content, 'html.parser')
            news_list = []
            processed_urls = set()
            
            # ë” í¬ê´„ì ì¸ ì…€ë ‰í„°ë¡œ ë‰´ìŠ¤ ë§í¬ ì°¾ê¸°
            selectors = [
                'a[href*="/News/"]', 'a[href*="/news/"]', 'a[href*="/article/"]',
                'a[href*="/story/"]', 'a[href*="/view/"]', 'a[href*="/read/"]',
                '.news-item a', '.article-item a', 'article a',
                '.list-item a', '.item a', '[class*="news"] a',
                '[class*="article"] a', '[class*="story"] a',
                'h1 a', 'h2 a', 'h3 a', 'h4 a'
            ]
            
            print(f"ğŸ” {len(selectors)}ê°œ ì…€ë ‰í„°ë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
            
            for i, selector in enumerate(selectors):
                try:
                    links = soup.select(selector)
                    print(f"ì…€ë ‰í„° {i+1}/{len(selectors)}: '{selector}' -> {len(links)}ê°œ ë§í¬ ë°œê²¬")
                    
                    if links:
                        for link in links[:20]:  # ìµœëŒ€ 20ê°œê¹Œì§€
                            try:
                                href = link.get('href')
                                if not href:
                                    continue
                                
                                # URL ì •ê·œí™”
                                if href.startswith('/'):
                                    href = source['base_url'].rstrip('/') + href
                                elif not href.startswith('http'):
                                    href = source['base_url'] + href
                                
                                if href in processed_urls:
                                    continue
                                processed_urls.add(href)
                                
                                # ì œëª© ì¶”ì¶œ
                                title = link.get_text(strip=True)
                                if not title:
                                    title_elem = link.find(['h1', 'h2', 'h3', 'h4', 'span', 'div', 'strong'])
                                    if title_elem:
                                        title = title_elem.get_text(strip=True)
                                
                                if title and len(title) > 5:
                                    news_list.append({
                                        'title': title,
                                        'url': href,
                                        'category': category,
                                        'source_name': source['source_name']
                                    })
                                    print(f"ğŸ“° ë‰´ìŠ¤ ì¶”ê°€: {title[:50]}...")
                                    
                                    if len(news_list) >= 15:  # ìµœëŒ€ 15ê°œ
                                        break
                            except Exception as e:
                                continue
                    
                    if news_list:
                        print(f"âœ… {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
                        break
                        
                except Exception as e:
                    print(f"ì…€ë ‰í„° {selector} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            return news_list
            
        except Exception as e:
            print(f"âŒ requests ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []
    
    def _scrape_with_selenium(self, source, category):
        """Seleniumì„ ì‚¬ìš©í•œ ìŠ¤í¬ë˜í•‘ - ì°¸ê³ í”„ë¡œì íŠ¸ ê¸°ë°˜ ê°œì„ """
        try:
            url = source['url']
            print(f"ğŸŒ Seleniumìœ¼ë¡œ {url} ì ‘ì† ì¤‘...")
            
            # Chrome ì˜µì…˜ ì„¤ì • (ì°¸ê³ í”„ë¡œì íŠ¸ ê¸°ë°˜)
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # WebDriver ì´ˆê¸°í™” (WinError 193 í•´ê²°)
            driver = None
            
            # ë°©ë²• 1: ì§ì ‘ Chrome ì‹¤í–‰
            try:
                driver = webdriver.Chrome(options=chrome_options)
                print("âœ… Chrome WebDriver ì§ì ‘ ì‹¤í–‰ ì„±ê³µ")
            except Exception as e:
                print(f"âŒ Chrome WebDriver ì§ì ‘ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                
                # ë°©ë²• 2: WebDriverManager ì‚¬ìš©
                try:
                    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
                    print("âœ… WebDriverManagerë¡œ ì‹¤í–‰ ì„±ê³µ")
                except Exception as e2:
                    print(f"âŒ WebDriverManagerë„ ì‹¤íŒ¨: {e2}")
                    
                    # ë°©ë²• 3: ì‹œìŠ¤í…œ PATHì˜ chromedriver ì‚¬ìš©
                    try:
                        driver = webdriver.Chrome(service=Service(), options=chrome_options)
                        print("âœ… ì‹œìŠ¤í…œ PATHì˜ chromedriver ì‚¬ìš© ì„±ê³µ")
                    except Exception as e3:
                        print(f"âŒ ëª¨ë“  WebDriver ì´ˆê¸°í™” ë°©ë²• ì‹¤íŒ¨: {e3}")
                        return []
            
            try:
                driver.get(url)
                time.sleep(5)  # JS ë Œë”ë§ ëŒ€ê¸°
                print(f"âœ… í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ: {url}")
                
                # ìŠ¤í¬ë¡¤í•˜ì—¬ ë™ì  ì½˜í…ì¸  ë¡œë“œ
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(3)
                driver.execute_script("window.scrollTo(0, 0);")
                time.sleep(2)
                
                news_list = []
                processed_urls = set()
                
                # ì°¸ê³ í”„ë¡œì íŠ¸ ê¸°ë°˜ ì…€ë ‰í„° (ì‚¬ì´íŠ¸ë³„ ìµœì í™”)
                site_selectors = {
                    'ì—°í•©ë‰´ìŠ¤': [
                        '//ul/li//strong/a',  # XPath ë°©ì‹
                        '//ul/li//a',
                        '.news-con a',
                        'article a'
                    ],
                    'ZDNet': [
                        '.newsPost a',  # ì°¸ê³ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•œ ì…€ë ‰í„°
                        '.newsPost h3 a',
                        'article a'
                    ],
                    'í•œêµ­ì¼ë³´': [
                        '.news-item a',
                        'article a',
                        '.list-item a'
                    ],
                    'ì¡°ì„ ì¼ë³´': [
                        '.story-item a',
                        'article a',
                        '.list-item a'
                    ],
                    'ì¤‘ì•™ì¼ë³´': [
                        '.story-item a',
                        'article a',
                        '.list-item a'
                    ]
                }
                
                # ì‚¬ì´íŠ¸ë³„ ìµœì í™”ëœ ì…€ë ‰í„° ì‚¬ìš©
                selectors = site_selectors.get(source['source_name'], [
                    'a[href*="/News/"]', 'a[href*="/news/"]', 'a[href*="/article/"]',
                    '.news-item a', '.article-item a', 'article a',
                    '.list-item a', '.item a', '[class*="news"] a'
                ])
                
                print(f"ğŸ” {len(selectors)}ê°œ ì…€ë ‰í„°ë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
                
                for i, selector in enumerate(selectors):
                    try:
                        if selector.startswith('//'):
                            # XPath ì‚¬ìš©
                            elements = driver.find_elements(By.XPATH, selector)
                        else:
                            # CSS ì…€ë ‰í„° ì‚¬ìš©
                            elements = driver.find_elements(By.CSS_SELECTOR, selector)
                        
                        print(f"ì…€ë ‰í„° {i+1}/{len(selectors)}: '{selector}' -> {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
                        
                        for element in elements[:20]:  # ìµœëŒ€ 20ê°œ
                            try:
                                href = element.get_attribute('href')
                                if not href or href in processed_urls:
                                    continue
                                
                                processed_urls.add(href)
                                
                                # URL ì •ê·œí™”
                                if href.startswith('/'):
                                    href = source['base_url'].rstrip('/') + href
                                elif not href.startswith('http'):
                                    href = source['base_url'] + href
                                
                                # ì œëª© ì¶”ì¶œ (ì°¸ê³ í”„ë¡œì íŠ¸ ë°©ì‹)
                                title = element.text.strip()
                                if not title:
                                    try:
                                        # h3 íƒœê·¸ì—ì„œ ì œëª© ì°¾ê¸° (ZDNet ë°©ì‹)
                                        title_elem = element.find_element(By.TAG_NAME, 'h3')
                                        title = title_elem.text.strip()
                                    except:
                                        try:
                                            # strong íƒœê·¸ì—ì„œ ì œëª© ì°¾ê¸°
                                            title_elem = element.find_element(By.TAG_NAME, 'strong')
                                            title = title_elem.text.strip()
                                        except:
                                            continue
                                
                                if title and len(title) > 5:
                                    news_list.append({
                                        'title': title,
                                        'url': href,
                                        'category': category,
                                        'source_name': source['source_name']
                                    })
                                    print(f"ğŸ“° ë‰´ìŠ¤ ì¶”ê°€: {title[:50]}...")
                                    
                                    if len(news_list) >= 15:  # ìµœëŒ€ 15ê°œ
                                        break
                            except Exception as e:
                                continue
                        
                        if news_list:
                            print(f"âœ… {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
                            break
                            
                    except Exception as e:
                        print(f"ì…€ë ‰í„° {selector} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
                        
            finally:
                driver.quit()
                print("ğŸ”š WebDriver ì¢…ë£Œ")
                
            return news_list
            
        except Exception as e:
            print(f"âŒ Selenium ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []
    
    def _get_sample_news(self, category):
        """ìƒ˜í”Œ ë‰´ìŠ¤ ë°ì´í„°"""
        sample_news = {
            "ì •ì¹˜": [
                {"title": "êµ­íšŒ ì˜ˆì‚°ì•ˆ ì‹¬ì˜ ì§„í–‰ ìƒí™©", "url": f"https://example.com/news/1", "category": "ì •ì¹˜", "source_name": "ìƒ˜í”Œ"},
                {"title": "ì •ì¹˜ê°œí˜ ê´€ë ¨ ë…¼ì˜ í™œë°œ", "url": f"https://example.com/news/2", "category": "ì •ì¹˜", "source_name": "ìƒ˜í”Œ"},
                {"title": "ì—¬ì•¼ ê°„ ì •ì±… í˜‘ì˜ ì§€ì†", "url": f"https://example.com/news/3", "category": "ì •ì¹˜", "source_name": "ìƒ˜í”Œ"},
                {"title": "ì§€ë°©ì„ ê±° ì¤€ë¹„ ë³¸ê²©í™”", "url": f"https://example.com/news/4", "category": "ì •ì¹˜", "source_name": "ìƒ˜í”Œ"},
                {"title": "êµ­ì •ê°ì‚¬ ê²°ê³¼ ë°œí‘œ", "url": f"https://example.com/news/5", "category": "ì •ì¹˜", "source_name": "ìƒ˜í”Œ"}
            ],
            "ê²½ì œ": [
                {"title": "ì£¼ì‹ì‹œì¥ ë³€ë™ì„± ì¦ê°€", "url": f"https://example.com/news/1", "category": "ê²½ì œ", "source_name": "ìƒ˜í”Œ"},
                {"title": "ë¶€ë™ì‚° ì‹œì¥ ë™í–¥ ë¶„ì„", "url": f"https://example.com/news/2", "category": "ê²½ì œ", "source_name": "ìƒ˜í”Œ"},
                {"title": "ê¸°ì—… ì‹¤ì  ë°œí‘œ ì‹œì¦Œ", "url": f"https://example.com/news/3", "category": "ê²½ì œ", "source_name": "ìƒ˜í”Œ"},
                {"title": "í™˜ìœ¨ ë³€ë™ ì˜í–¥ ë¶„ì„", "url": f"https://example.com/news/4", "category": "ê²½ì œ", "source_name": "ìƒ˜í”Œ"},
                {"title": "ê²½ì œ ì§€í‘œ ë°œí‘œ", "url": f"https://example.com/news/5", "category": "ê²½ì œ", "source_name": "ìƒ˜í”Œ"}
            ]
        }
        
        return sample_news.get(category, [
            {"title": f"{category} ê´€ë ¨ ë‰´ìŠ¤ 1", "url": f"https://example.com/news/1", "category": category, "source_name": "ìƒ˜í”Œ"},
            {"title": f"{category} ê´€ë ¨ ë‰´ìŠ¤ 2", "url": f"https://example.com/news/2", "category": category, "source_name": "ìƒ˜í”Œ"},
            {"title": f"{category} ê´€ë ¨ ë‰´ìŠ¤ 3", "url": f"https://example.com/news/3", "category": category, "source_name": "ìƒ˜í”Œ"},
            {"title": f"{category} ê´€ë ¨ ë‰´ìŠ¤ 4", "url": f"https://example.com/news/4", "category": category, "source_name": "ìƒ˜í”Œ"},
            {"title": f"{category} ê´€ë ¨ ë‰´ìŠ¤ 5", "url": f"https://example.com/news/5", "category": category, "source_name": "ìƒ˜í”Œ"}
        ])
