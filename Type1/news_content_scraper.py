"""
ë‰´ìŠ¤ URLì˜ ì „ì²´ ë‚´ìš©ì„ ìŠ¤í¬ë˜í•‘í•˜ëŠ” ëª¨ë“ˆ
"""
import requests
from bs4 import BeautifulSoup
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
import re

class NewsContentScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def scrape_news_content(self, url):
        """ë‰´ìŠ¤ URLì˜ ì „ì²´ ë‚´ìš©ì„ ìŠ¤í¬ë˜í•‘"""
        try:
            print(f"ğŸ“° ë‰´ìŠ¤ ë‚´ìš© ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")
            
            # 1ë‹¨ê³„: requests + BeautifulSoup ì‹œë„
            content = self._scrape_with_requests(url)
            if content:
                return content
            
            # 2ë‹¨ê³„: Selenium ì‹œë„
            content = self._scrape_with_selenium(url)
            if content:
                return content
            
            return None
            
        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ ë‚´ìš© ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return None
    
    def _scrape_with_requests(self, url):
        """requestsë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ë‚´ìš© ìŠ¤í¬ë˜í•‘"""
        try:
            print(f"ğŸ“¡ requestsë¡œ {url} ì ‘ì† ì¤‘...")
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì‚¬ì´íŠ¸ë³„ ìµœì í™”ëœ ì…€ë ‰í„°
            content_selectors = {
                'í•œêµ­ì¼ë³´': [
                    '.news-content', '.article-content', '.content',
                    'article .text', '.news-text', '.article-text'
                ],
                'ì—°í•©ë‰´ìŠ¤': [
                    '.news-con', '.article-content', '.content',
                    'article .text', '.news-text', '.article-text'
                ],
                'ZDNet': [
                    '.newsPost .content', '.article-content', '.content',
                    'article .text', '.news-text', '.article-text'
                ],
                'ì¡°ì„ ì¼ë³´': [
                    '.story-content', '.article-content', '.content',
                    'article .text', '.news-text', '.article-text'
                ],
                'ì¤‘ì•™ì¼ë³´': [
                    '.story-content', '.article-content', '.content',
                    'article .text', '.news-text', '.article-text'
                ]
            }
            
            # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ë‚´ìš© ì…€ë ‰í„°
            general_selectors = [
                'article', '.article-content', '.news-content', '.content',
                '.story-content', '.post-content', '.entry-content',
                '[class*="article"]', '[class*="content"]', '[class*="story"]',
                'main', '.main-content', '.text-content'
            ]
            
            # ëª¨ë“  ì…€ë ‰í„° ì‹œë„
            all_selectors = []
            for site_selectors in content_selectors.values():
                all_selectors.extend(site_selectors)
            all_selectors.extend(general_selectors)
            
            content_text = ""
            
            for selector in all_selectors:
                try:
                    elements = soup.select(selector)
                    if elements:
                        for element in elements:
                            text = element.get_text(strip=True)
                            if len(text) > 100:  # ì¶©ë¶„í•œ ê¸¸ì´ì˜ í…ìŠ¤íŠ¸ë§Œ
                                content_text = text
                                print(f"âœ… ë‰´ìŠ¤ ë‚´ìš© ë°œê²¬ (ì…€ë ‰í„°: {selector}): {len(text)}ì")
                                break
                        if content_text:
                            break
                except Exception as e:
                    continue
            
            if content_text:
                # í…ìŠ¤íŠ¸ ì •ë¦¬
                content_text = self._clean_text(content_text)
                return {
                    'title': self._extract_title(soup),
                    'content': content_text,
                    'url': url,
                    'method': 'requests'
                }
            
            return None
            
        except Exception as e:
            print(f"âŒ requests ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return None
    
    def _scrape_with_selenium(self, url):
        """Seleniumì„ ì‚¬ìš©í•œ ë‰´ìŠ¤ ë‚´ìš© ìŠ¤í¬ë˜í•‘"""
        try:
            print(f"ğŸŒ Seleniumìœ¼ë¡œ {url} ì ‘ì† ì¤‘...")
            
            # Chrome ì˜µì…˜ ì„¤ì •
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # WebDriver ì´ˆê¸°í™”
            driver = None
            try:
                driver = webdriver.Chrome(options=chrome_options)
            except Exception as e:
                try:
                    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
                except Exception as e2:
                    print(f"âŒ WebDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e2}")
                    return None
            
            try:
                driver.get(url)
                time.sleep(5)  # JS ë Œë”ë§ ëŒ€ê¸°
                print(f"âœ… í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ: {url}")
                
                # ë‰´ìŠ¤ ë‚´ìš© ì…€ë ‰í„°ë“¤
                content_selectors = [
                    'article', '.article-content', '.news-content', '.content',
                    '.story-content', '.post-content', '.entry-content',
                    '[class*="article"]', '[class*="content"]', '[class*="story"]',
                    'main', '.main-content', '.text-content'
                ]
                
                content_text = ""
                title = ""
                
                for selector in content_selectors:
                    try:
                        elements = driver.find_elements(By.CSS_SELECTOR, selector)
                        if elements:
                            for element in elements:
                                text = element.text.strip()
                                if len(text) > 100:  # ì¶©ë¶„í•œ ê¸¸ì´ì˜ í…ìŠ¤íŠ¸ë§Œ
                                    content_text = text
                                    print(f"âœ… ë‰´ìŠ¤ ë‚´ìš© ë°œê²¬ (ì…€ë ‰í„°: {selector}): {len(text)}ì")
                                    break
                            if content_text:
                                break
                    except Exception as e:
                        continue
                
                # ì œëª© ì¶”ì¶œ
                try:
                    title_element = driver.find_element(By.TAG_NAME, 'h1')
                    title = title_element.text.strip()
                except:
                    try:
                        title_element = driver.find_element(By.CSS_SELECTOR, '.title, .headline, h1, h2')
                        title = title_element.text.strip()
                    except:
                        title = "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
                if content_text:
                    # í…ìŠ¤íŠ¸ ì •ë¦¬
                    content_text = self._clean_text(content_text)
                    return {
                        'title': title,
                        'content': content_text,
                        'url': url,
                        'method': 'selenium'
                    }
                
                return None
                
            finally:
                driver.quit()
                print("ğŸ”š WebDriver ì¢…ë£Œ")
                
        except Exception as e:
            print(f"âŒ Selenium ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_title(self, soup):
        """BeautifulSoupì—ì„œ ì œëª© ì¶”ì¶œ"""
        try:
            # ë‹¤ì–‘í•œ ì œëª© ì…€ë ‰í„° ì‹œë„
            title_selectors = [
                'h1', '.title', '.headline', '.article-title', '.news-title',
                'title', '.post-title', '.entry-title'
            ]
            
            for selector in title_selectors:
                element = soup.select_one(selector)
                if element:
                    title = element.get_text(strip=True)
                    if title and len(title) > 5:
                        return title
            
            return "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        except:
            return "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    def _clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
        text = re.sub(r'[^\w\sê°€-í£.,!?]', '', text)
        # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì œê±°
        text = re.sub(r'\n+', '\n', text)
        return text.strip()
